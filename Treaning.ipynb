{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gnKyNhy4Sw0P"
      },
      "source": [
        "In this version,\n",
        "- Training pipline of the selection mechanism\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "X0WJjFZ_mhqu",
        "outputId": "478861fb-664f-48a2-c73a-75e270b07b17"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "print(\"Tensorflow version: \",tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Checking if cuda is there.\n",
        "print(\"Cuda Availability: \", tf.test.is_built_with_cuda())\n",
        "\n",
        "# Checking GPU is available or not.\n",
        "print(\"GPU  Availability: \", tf.test.is_gpu_available())\n",
        "\n",
        "# Check nos of GPUS\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "m_dPbml8d8FP"
      },
      "source": [
        "# Data Downloading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-lJiLRCeT1-",
        "outputId": "4fe0d3a1-1601-4748-b7d0-43a0325910f4"
      },
      "outputs": [],
      "source": [
        "# choose the dataset version and paths\n",
        "# 'modelnet40v1' or 'modelnet40v2' or 'shaded_modelnet40v2'\n",
        "##########################\n",
        "dataset_version= 'modelnet40v1' \n",
        "dataset_train = './data/modelnet40v1_train'\n",
        "\n",
        "#dataset_version= 'modelnet40v2'\n",
        "#dataset_train = './data/modelnet40v2_train'\n",
        "\n",
        "#dataset_version= 'shaded_modelnet40v2'\n",
        "#dataset_train = './data/modelnet40v2_train'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# spicify the img size (here 224*224)\n",
        "Img_Size= 224 "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "us_3RIAGebLJ"
      },
      "source": [
        "# Data Processing\n",
        "\n",
        "\n",
        "We've included helper functions that will label images, convert them into arrays, and then finally into a generator that will enable them to be loaded into the model in batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BPK8hfbbecMu"
      },
      "outputs": [],
      "source": [
        "# for training and testing\n",
        "##########################\n",
        "\n",
        "import random #Python Random module is an in-built module of Python which is used to generate random numbers.\n",
        "from random import shuffle #is an inbuilt method of the random module. It is used to shuffle a sequence (list). Shuffling a list of objects means changing the position of the elements of the sequence\n",
        "import cv2 # OpenCV-Python is a library of Python bindings designed to solve computer vision problems.\n",
        "import os # Python OS module provides the facility to establish the interaction between the user and the operating system. It offers many useful OS functions that are used to perform OS-based tasks and get related information about operating system.\n",
        "from tqdm import tqdm # is a library in Python which is used for creating Progress Meters or Progress Bars. tqdm got its name from the Arabic name taqaddum which means 'progress'\n",
        "import matplotlib.pyplot as plt #Matplotlib is a comprehensive library for creating static, animated, and interactive visualizations in Python. Matplotlib makes easy things easy and hard things possible. Create publication quality plots. Make interactive figures that can zoom, pan, update. Customize visual style and layout.\n",
        "import numpy as np #NumPy (Numerical Python) is an open-source library for the Python programming language. It is used for scientific computing and working with arrays.\n",
        "from tensorflow.python.keras.utils.data_utils import Sequence #Keras and TensorFlow are open source Python libraries for working with neural networks, creating machine learning models and performing deep learning\n",
        "# keras.utils This package provides utilities for Keras, such as modified callbacks, genereators, etc.\n",
        "import tensorflow as tf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "LWhHWkkfVaAI"
      },
      "outputs": [],
      "source": [
        "# for training and testing\n",
        "##########################\n",
        "\n",
        "#For shuffling or any thing random\n",
        "import random #Python Random module is an in-built module of Python which is used to generate random numbers.\n",
        "from random import shuffle #is an inbuilt method of the random module. It is used to shuffle a sequence (list). Shuffling a list of objects means changing the position of the elements of the sequence\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdSqKQdR5pVS",
        "outputId": "435b9851-0094-4468-9e4e-5ef7062ba3b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of views is 20\n"
          ]
        }
      ],
      "source": [
        "# for training and testing\n",
        "##########################\n",
        "\n",
        "global num_views\n",
        "\n",
        "if dataset_version == 'modelnet40v1':\n",
        "    num_views = 12\n",
        "elif dataset_version == 'modelnet40v2':\n",
        "    num_views = 20\n",
        "elif dataset_version == 'shaded_modelnet40v1':\n",
        "    num_views = 12\n",
        "\n",
        "print (\"number of views is \" + str(num_views))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFhWO9hL3Q2L"
      },
      "outputs": [],
      "source": [
        "# for training and testing\n",
        "##########################\n",
        "\n",
        "\n",
        "def extract_View_No(img): # function to extract the view no\n",
        "  ## Helper for process_data\n",
        "  ViewNo =img[-6:-4] #take the last 2 characters from the img name before the \".png\"\n",
        "  if ViewNo == '01': return 1\n",
        "  elif ViewNo == '02': return 2\n",
        "  elif ViewNo == '03': return 3\n",
        "  elif ViewNo == '04': return 4\n",
        "  elif ViewNo == '05': return 5\n",
        "  elif ViewNo == '06': return 6\n",
        "  elif ViewNo == '07': return 7\n",
        "  elif ViewNo == '08': return 8\n",
        "  elif ViewNo == '09': return 9\n",
        "  elif ViewNo == '10': return 10\n",
        "  elif ViewNo == '11': return 11\n",
        "  elif ViewNo == '12': return 12\n",
        "  elif ViewNo == '13': return 13\n",
        "  elif ViewNo == '14': return 14\n",
        "  elif ViewNo == '15': return 15\n",
        "  elif ViewNo == '16': return 16\n",
        "  elif ViewNo == '17': return 17\n",
        "  elif ViewNo == '18': return 18\n",
        "  elif ViewNo == '19': return 19\n",
        "  elif ViewNo == '20': return 20\n",
        "  #print(ViewNo)\n",
        "\n",
        "\n",
        "def label_image_encoder(img): # function to label image\n",
        "  ## Helper for process_data\n",
        "  label = img.split('_')[0] # from the image name, take the part before \"_\" and consider it as label #split() method is a beneficial tool for manipulating strings. It returns a list of strings after the main string is separated by a delimiter.\n",
        "  if label == 'airplane': return 0 # encode airplane label as 0\n",
        "  elif label == 'bathtub': return 1 # encode bathtub label as 1\n",
        "  elif label == 'bed': return 2 # encode bed label as 2\n",
        "  elif label == 'bench': return 3 # encode bench label as 3\n",
        "  elif label == 'bookshelf': return 4 # encode bookshelf label as 4\n",
        "  elif label == 'bottle': return 5 # encode bottle label as 5\n",
        "  elif label == 'bowl': return 6 # encode bowl label as 6\n",
        "  elif label == 'car': return 7 # encode car label as 7\n",
        "  elif label == 'chair': return 8 # encode chair label as 8\n",
        "  elif label == 'cone': return 9 # encode cone label as 9\n",
        "  elif label == 'cup': return 10 # encode cup label as 10\n",
        "  elif label == 'curtain': return 11 # encode curtain label as 11\n",
        "  elif label == 'desk': return 12 # encode desk label as 12\n",
        "  elif label == 'door': return 13 # encode door label as 13\n",
        "  elif label == 'dresser': return 14 # encode dresser label as 14\n",
        "  elif label == 'flower': return 15 # encode floer_pot label as 15\n",
        "  elif label == 'glass': return 16 # encode glass_box label as 16\n",
        "  elif label == 'guitar': return 17 # encode guitar label as 17\n",
        "  elif label == 'keyboard': return 18 # encode keyboard label as 18\n",
        "  elif label == 'lamp': return 19 # encode lamp label as 19\n",
        "  elif label == 'laptop': return 20 # encode laptop label as 20\n",
        "  elif label == 'mantel': return 21 # encode mantel label as 21\n",
        "  elif label == 'monitor': return 22 # encode monitor label as 22\n",
        "  elif label == 'night': return 23 # encode night_stand label as 23\n",
        "  elif label == 'person': return 24 # encode person label as 24\n",
        "  elif label == 'piano': return 25 # encode piano label as 25\n",
        "  elif label == 'plant': return 26 # encode plant label as 26\n",
        "  elif label == 'radio': return 27 # encode radio label as 27\n",
        "  elif label == 'range': return 28 # encode range_hood label as 28\n",
        "  elif label == 'sink': return 29 # encode sink label as 29\n",
        "  elif label == 'sofa': return 30 # encode sofa label as 30\n",
        "  elif label == 'stairs': return 31 # encode stairs label as 31\n",
        "  elif label == 'stool': return 32 # encode stool label as 32\n",
        "  elif label == 'table': return 33 # encode table label as 33\n",
        "  elif label == 'tent': return 34 # encode tent label as 34\n",
        "  elif label == 'toilet': return 35 # encode toilet label as 35\n",
        "  elif label == 'tv': return 36 # encode tv_stand label as 36\n",
        "  elif label == 'vase': return 37 # encode vase label as 37\n",
        "  elif label == 'wardrobe': return 38 # encode wardrobe label as 38\n",
        "  elif label == 'xbox': return 39 # encode xbox label as 39\n",
        "\n",
        "\n",
        "def process_data(image_list, DATA_FOLDER, IMG_SIZE): # function that take the images, the data folder name, and the wanted image size, and retun the list of images as arrays with there labels and paths\n",
        "  ## Helper for manual_pre_process\n",
        "  ## Creates an array of images, labels, and file path\n",
        "  data_df = [] #[] is a list: A multible collection of values, here define new array\n",
        "  for img in tqdm(image_list): # tqdm . It will display a progress bar of the for loop, for each image in image_list do the following\n",
        "    path = os.path.join(DATA_FOLDER, img) # concatenates various path components with exactly one directory separator ('/'), concatenate DATA_FOLDER path then / then the image name to create an image's path\n",
        "    ViewNo = extract_View_No(img) # extract the view no\n",
        "    label = label_image_encoder(img) # use the above function to encode the image's label\n",
        "    img = cv2.imread(path, cv2.IMREAD_COLOR) # cv2. imread() method loads an image from the specified file. IMREAD_COLOR reads the image with RGB colors but no transparency channel, load image from its path\n",
        "    img = cv2.resize(img, (IMG_SIZE, IMG_SIZE)) # Resizing the image img, by only change the width=IMG_SIZE and height=IMG_SIZE of the image, this step to make all image with the same size\n",
        "    #data_df.append([np.array(img), np.array(label), path]) #Append in Python is a pre-defined method used to add a single item to certain collection types. so, append the list data_df by adding sum-list with 3 element for each image [np.array(img)=image as array, np.array(label)=image's label as 0 or 1, path=image's path]it\n",
        "    data_df.append([np.array(img), np.array(label), ViewNo, path]) #Append in Python is a pre-defined method used to add a single item to certain collection types. so, append the list data_df by adding sum-list with 3 element for each image [np.array(img)=image as array, np.array(label)=image's label as 0 or 1, path=image's path]it\n",
        "\n",
        "  return data_df # return the data_df list after process it\n",
        "\n",
        "\n",
        "def manual_pre_process(dir, IMG_SIZE):\n",
        "  '''\n",
        "  Creates an array of images, labels, and files from a directory of image files\n",
        "\n",
        "  Args:\n",
        "    dir: string, folder name\n",
        "    IMG_SIZE: int, image height and width\n",
        "\n",
        "  Returns\n",
        "    X: (n x IMG_SIZE x IMG_SIZE) numpy array of images\n",
        "    y: (n,) numpy array of labels\n",
        "    files: (n,) numpy array of files\n",
        "\n",
        "  '''\n",
        "  image_lst = sorted(os.listdir(dir)) # get the list of all files and directories in the specified directory \"dir\"\n",
        "  data_df = process_data(image_lst, dir, IMG_SIZE) # take the images, the data folder name, and the wanted image size, and retun the list of images after processing as arrays with there labels and paths\n",
        "  X = np.array([i[0] for i in data_df]).reshape(-1, IMG_SIZE, IMG_SIZE, 3) # take the images only (as arrays) from data_df and save them as X (the input images)\n",
        "  y = np.array([i[1] for i in data_df]) # take the labels only from data_df and save them as Y (the actual labels)\n",
        "  ViewNos = np.array([i[2] for i in data_df]) # take the ViewNo only from data_df and save them as ViewNo\n",
        "  files = np.array([i[3] for i in data_df]) # take the images' paths only from data_df and save them as files\n",
        "  return X, y, ViewNos, files # return X=the images(as arrays), y=the actual labels, and files=the images' paths\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_i-a6x51er32",
        "outputId": "e7ac0421-9f99-4df3-e46c-9f8b2a4ebd2e"
      },
      "outputs": [],
      "source": [
        "# for training and testing\n",
        "##########################\n",
        "\n",
        "\n",
        "# Dictionary\n",
        "class_info = {0: 'airplane',\n",
        "              1: 'bathtub',\n",
        "              2: 'bed',\n",
        "              3: 'bench',\n",
        "              4: 'bookshelf',\n",
        "              5: 'bottle',\n",
        "              6: 'bowl',\n",
        "              7: 'car',\n",
        "              8: 'chair',\n",
        "              9: 'cone',\n",
        "              10: 'cup',\n",
        "              11: 'curtain',\n",
        "              12: 'desk',\n",
        "              13: 'door',\n",
        "              14: 'dresser',\n",
        "              15: 'flower_pot',\n",
        "              16: 'glass_box',\n",
        "              17: 'guitar',\n",
        "              18: 'keyboard',\n",
        "              19: 'lamp',\n",
        "              20: 'laptop',\n",
        "              21: 'mantel',\n",
        "              22: 'monitor',\n",
        "              23: 'night_stand',\n",
        "              24: 'person',\n",
        "              25: 'piano',\n",
        "              26: 'plant',\n",
        "              27: 'radio',\n",
        "              28: 'range_hood',\n",
        "              29: 'sink',\n",
        "              30: 'sofa',\n",
        "              31: 'stairs',\n",
        "              32: 'stool',\n",
        "              33: 'table',\n",
        "              34: 'tent',\n",
        "              35: 'toilet',\n",
        "              36: 'tv_stand',\n",
        "              37: 'vase',\n",
        "              38: 'wardrobe',\n",
        "              39: 'xbox'} # define a dictionary (a key/value mapping) of the labels\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X_train, y_train, ViewNo_train, files_train = manual_pre_process(dataset_train, Img_Size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for training and testing\n",
        "##########################\n",
        "\n",
        "from keras.utils.np_utils import to_categorical\n",
        "categorical_y_train = to_categorical(y_train, 40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "109n588Tgb0B",
        "outputId": "9ba59d2d-4d19-4f96-f099-624191df2658"
      },
      "outputs": [],
      "source": [
        "#for training\n",
        "########################\n",
        "\n",
        "train_NoOfSamples, h, w, channel= y_train.shape[0], 224, 224, 3\n",
        "\n",
        "print(\"Number of training Samples is\")\n",
        "print(train_NoOfSamples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 903
        },
        "id": "rnbILyK5ewEI",
        "outputId": "65fff799-7eb4-4c8e-92d7-8cd4f52c4838"
      },
      "outputs": [],
      "source": [
        "# for training\n",
        "##########################\n",
        "# Note: spicify the subplot rows and columns based on the number of views\n",
        "##########################\n",
        "\n",
        "\n",
        "# show the (12 or 20 views) of one object samples before feature extraction\n",
        "plt.figure(figsize=(15, 15)) # create a fig of size 15*15\n",
        "\n",
        "\n",
        "for i in range(0,num_views):\n",
        " img = X_train[i] #take sample image\n",
        " label = y_train[i] #take the label of the sample image\n",
        " viewNo= ViewNo_train[i]\n",
        " path = files_train[i] #take the path of the sample image\n",
        "\n",
        " ax = plt.subplot(4, 5, i +1) # for 20 views\n",
        " #ax = plt.subplot(3, 4, i +1) # For 12 views\n",
        " plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB)) # show the img in the plot\n",
        " plt.axis('off') # remove the axis\n",
        " category = class_info[label.item()] # convert the label from ndarray to int then bring its class\n",
        " title= category.capitalize() + ', View No:' + str(viewNo)\n",
        " plt.title(title) # show the title above the img"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "b3bsGDtokjiB"
      },
      "source": [
        "# Training the Pre-trained Model (Stage 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "thhZjqz1fVmc"
      },
      "outputs": [],
      "source": [
        "# for training and testing\n",
        "##########################\n",
        "\n",
        "# Loading the Pre-Trained Model\n",
        "# Upload the required libraries\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
        "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
        "import time\n",
        "from keras.models import load_model\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from tqdm import tqdm\n",
        "\n",
        "# keras imports\n",
        "from keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from keras.applications.vgg19 import VGG19, preprocess_input\n",
        "from keras.applications.xception import Xception, preprocess_input\n",
        "from keras.applications.inception_resnet_v2 import InceptionResNetV2, preprocess_input\n",
        "from keras.applications.mobilenet import MobileNet, preprocess_input\n",
        "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
        "from keras.applications.densenet import DenseNet121\n",
        "from keras.applications.densenet import DenseNet169\n",
        "from keras.applications.densenet import DenseNet201\n",
        "from keras.applications.nasnet import NASNetLarge\n",
        "from keras.applications.nasnet import NASNetMobile\n",
        "from keras.applications.mobilenet_v2 import MobileNetV2\n",
        "from keras.applications.resnet import ResNet50\n",
        "from keras.applications.resnet import ResNet101\n",
        "from keras.applications.resnet import ResNet152\n",
        "from keras.applications.resnet_v2 import ResNet50V2\n",
        "from keras.applications.resnet_v2 import ResNet101V2\n",
        "from keras.applications.resnet_v2 import ResNet152V2\n",
        "\n",
        "# filter warnings\n",
        "import warnings\n",
        "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
        "from keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from keras.applications.vgg19 import VGG19, preprocess_input\n",
        "from keras.applications.xception import Xception, preprocess_input\n",
        "from keras.applications.inception_resnet_v2 import InceptionResNetV2, preprocess_input\n",
        "from keras.applications.mobilenet import MobileNet, preprocess_input\n",
        "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
        "from keras.applications.efficientnet import EfficientNetB0, preprocess_input\n",
        "from keras.applications.efficientnet import EfficientNetB1, preprocess_input\n",
        "from keras.applications.efficientnet import EfficientNetB2, preprocess_input\n",
        "from keras.applications.efficientnet import EfficientNetB3, preprocess_input\n",
        "from keras.applications.efficientnet import EfficientNetB4, preprocess_input\n",
        "from keras.applications.efficientnet import EfficientNetB5, preprocess_input\n",
        "from keras.applications.efficientnet import EfficientNetB6, preprocess_input\n",
        "from keras.applications.efficientnet import EfficientNetB7, preprocess_input\n",
        "\n",
        "from keras.preprocessing import image\n",
        "from keras.models import Model\n",
        "from keras.models import model_from_json\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense,GlobalAveragePooling2D\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from pycm import *\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy import array\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "from keras_tqdm import TQDMNotebookCallback\n",
        "from keras_tqdm import TQDMCallback\n",
        "#from keras_preprocessing.image import load_img\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.optimizers import gradient_descent_v2\n",
        "from keras.optimizers import adam_v2\n",
        "from keras.layers import MaxPooling2D\n",
        "\n",
        "import keras\n",
        "from tensorflow import keras\n",
        "from keras import optimizers\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from keras.layers import Flatten\n",
        "from tensorflow.keras.layers import ( # from tensorflow.keras.layers \n",
        "     BatchNormalization, Flatten, Dropout, Dense #,BatchNormalization, SeparableConv2D, Activation, \n",
        ")\n",
        "from tensorflow.keras.utils import plot_model ## from tensorflow.keras.utils \n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "M87drTWpht5Q"
      },
      "outputs": [],
      "source": [
        "# for training and testing\n",
        "##########################\n",
        "\n",
        "\n",
        "def plot_acc_loss(history, PLOT_NAME,model_name_txt):\n",
        "    fig = plt.figure(figsize=(10,5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'])\n",
        "    #plt.plot(history.history['val_accuracy'])\n",
        "    plt.title('model accuracy')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'validation'], loc='upper left')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'])\n",
        "    #plt.plot(history.history['val_loss'])\n",
        "    plt.title('model loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'test'], loc='upper right')\n",
        "    plt.savefig(\"./Results/\"+str(dataset_version)+\"/\"+str(model_name_txt)+\"/\"+str(PLOT_NAME))\n",
        "    plt.show()\n",
        "\n",
        "def getPrediction(preds): # preds is the otput scors from softmax\n",
        "  im_class= np.argmax(preds, axis = 0) # the index as an array of one element , argmax Returns the indices of the maximum values along an axis\n",
        "  #\"the index of the class\"\n",
        "  idx= im_class\n",
        "  #\"the probability is\"\n",
        "  propability= preds[idx]\n",
        "  # the class label\n",
        "  label= class_info[idx]\n",
        "  return idx, label, propability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7rEa3J278ZX"
      },
      "outputs": [],
      "source": [
        "#all_deep_models = [VGG16, VGG19, MobileNet, InceptionV3, InceptionResNetV2, Xception, DenseNet121, DenseNet169, DenseNet201, NASNetMobile, MobileNetV2, ResNet50, ResNet101, ResNet152, ResNet50V2, ResNet101V2, ResNet152V2, NASNetLarge ]\n",
        "#all_model_name_txt = [\"VGG16\", \"VGG19\", \"MobileNet\", \"InceptionV3\", \"InceptionResNetV2\", \"Xception\", \"DenseNet121\", \"DenseNet169\", \"DenseNet201\", \"NASNetMobile\", \"MobileNetV2\",  \"ResNet50\", \"ResNet101\", \"ResNet152\", \"ResNet50V2\", \"ResNet101V2\", \"ResNet152V2\", \"NASNetLarge\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "xNfeMGxr8CDh"
      },
      "outputs": [],
      "source": [
        "# for training and testing\n",
        "##########################\n",
        "# Note: spicify the deep model name\n",
        "##########################\n",
        "\n",
        "\n",
        "all_deep_models = [ResNet152]\n",
        "all_model_name_txt = [\"ResNet152\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "knMv73HO86-b"
      },
      "outputs": [],
      "source": [
        "# for training and testing\n",
        "##########################\n",
        "# Note: spicify the BATCH_SIZE based on the number of views\n",
        "#       spicify the img size that expected by the deep model\n",
        "##########################\n",
        "\n",
        "\n",
        "CLASSES = 40\n",
        "WIDTH = Img_Size\n",
        "HEIGHT = Img_Size\n",
        "BATCH_SIZE = 384 #192 #20 shapes with a total of 400 views (20*20 =400) for 20-views version,  and 32 shapes with a total of 384 views (32*12 =384)) for 12-views version\n",
        "EPOCHS = 20 # 10\n",
        "learning_rate = 0.0001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import Sequence\n",
        "import numpy as np   \n",
        "\n",
        "class DataGenerator(Sequence):\n",
        "    def __init__(self, x_set, y_set, batch_size):\n",
        "        self.x, self.y = x_set, y_set\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        return batch_x, batch_y\n",
        "\n",
        "#test_gen = DataGenerator(X_test, y_test, 32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import Sequence\n",
        "import numpy as np   \n",
        "\n",
        "class DataGenerator1(Sequence):\n",
        "    def __init__(self, x_set, batch_size):\n",
        "        self.x = x_set\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        return batch_x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWswbM7n9sQQ",
        "outputId": "c56694c9-cbc1-4ead-8bf6-82a344e4e00d"
      },
      "outputs": [],
      "source": [
        "# for training\n",
        "##########################\n",
        "# Note: spicify the shape of the input\n",
        "##########################\n",
        "\n",
        "## pre-training\n",
        "\n",
        "\n",
        "\n",
        "i=0\n",
        "for deep_model in all_deep_models:\n",
        "    model_name = deep_model\n",
        "    model_name_txt = str(all_model_name_txt[i])\n",
        "    print (\"- Deep model: \"+str(model_name_txt))\n",
        "\n",
        "\n",
        "    FOLD_NAME = str(model_name_txt)+\".txt\"\n",
        "    Train_TimeFOLD_NAME = \"TrainingTime_stage1_\"+str(model_name_txt)+\".txt\"\n",
        "    PLOT_NAME = \"plot_acc_loss_stage1\"+\".png\"\n",
        "\n",
        "     # setup model\n",
        "    base_model = model_name(weights='imagenet', include_top=False, input_tensor=Input(shape=(Img_Size, Img_Size, 3)))\n",
        "    model= base_model\n",
        "\n",
        "\n",
        "\n",
        "    print (\"# Final model architecture\")\n",
        "    model.summary()\n",
        "\n",
        "    print (\"# Final model architecture plotted\")\n",
        "    plot_model(model, to_file='model_1.png', rankdir='TB',show_dtype=True, show_layer_names=True)\n",
        "    ploted_model_path = \"./data/model_1.png\"  # Image file path\n",
        "    ploted_model_img = Image.open(ploted_model_path)\n",
        "    ploted_model_new_path = \"./Results/\"+str(dataset_version)+\"/\"+str(model_name_txt)+\"/\"+str(model_name_txt)+\"_model_stage1.png\"\n",
        "    ploted_model_img.save(ploted_model_new_path) # Image saving to another directory\n",
        "\n",
        "\n",
        "    MODEL_FILE = \"./Results/\"+str(dataset_version)+\"/\"+str(model_name_txt)+\"/\"+str(model_name_txt)+\"_stage1.model\"\n",
        "    model.save(MODEL_FILE)\n",
        "    model_1 = load_model(MODEL_FILE)\n",
        "\n",
        "    i+=1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_1.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# extract and save the features incrementally as baches and clear the memory occupied by the previous batches.\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "\n",
        "# Constants\n",
        "batch_size = 100\n",
        "output_directory = \"./data/\"\n",
        "\n",
        "# Calculate the number of batches\n",
        "num_images = train_NoOfSamples\n",
        "num_batches = int(np.ceil(num_images / batch_size))\n",
        "\n",
        "# Initialize an empty list to store features\n",
        "all_features = []\n",
        "\n",
        "# Iterate through each batch\n",
        "#for i in range(1500, 1969):\n",
        "for i in range(num_batches):\n",
        "    start_index = i * batch_size\n",
        "    end_index = min((i + 1) * batch_size, num_images)\n",
        "\n",
        "    # Initialize an empty list to store features in the current batch\n",
        "    batch_features = []\n",
        "\n",
        "    # Save images in the current batch as features\n",
        "    for j in range(start_index, end_index):\n",
        "        batch_features.append(X_train[j])\n",
        "\n",
        "    # Convert batch features list to a NumPy array\n",
        "    batch_features = np.array(batch_features)\n",
        "\n",
        "    # Extract features of the the current batch using the model\n",
        "    batch_features = model_1.predict(batch_features, verbose=1) \n",
        "\n",
        "    # Append batch features to the list of all features\n",
        "    all_features.append(batch_features)\n",
        "\n",
        "    # Clear session to free up GPU memory\n",
        "    K.clear_session() # a K.clear_session() call after each batch to clear the TensorFlow session and free up GPU memory. \n",
        "\n",
        "    print(f\"Batch {i+1}/{num_batches} processed.\")\n",
        "\n",
        "    # Save features after every 10 batches (adjust as needed)\n",
        "    if (i+1) % 10 == 0 or (i+1) == num_batches:\n",
        "        # Concatenate all batches of features into a single NumPy array\n",
        "        all_features = np.concatenate(all_features)\n",
        "\n",
        "        # Save features to a separate file\n",
        "        output_file = f\"./data/features_batch_{i+1}.npy\"\n",
        "        np.save(output_file, all_features) # This allows you to save the features incrementally and clear the memory occupied by the previous batches.\n",
        "        # Clear the list of features to save memory\n",
        "        all_features = []\n",
        "\n",
        "print(\"Feature extraction completed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "features_batch_ 10  is appended\n",
            "features_batch_ 20  is appended\n",
            "features_batch_ 30  is appended\n",
            "features_batch_ 40  is appended\n",
            "features_batch_ 50  is appended\n",
            "features_batch_ 60  is appended\n",
            "features_batch_ 70  is appended\n",
            "features_batch_ 80  is appended\n",
            "features_batch_ 90  is appended\n",
            "features_batch_ 100  is appended\n",
            "features_batch_ 110  is appended\n",
            "features_batch_ 120  is appended\n",
            "features_batch_ 130  is appended\n",
            "features_batch_ 140  is appended\n",
            "features_batch_ 150  is appended\n",
            "features_batch_ 160  is appended\n",
            "features_batch_ 170  is appended\n",
            "features_batch_ 180  is appended\n",
            "features_batch_ 190  is appended\n",
            "features_batch_ 200  is appended\n",
            "features_batch_ 210  is appended\n",
            "features_batch_ 220  is appended\n",
            "features_batch_ 230  is appended\n",
            "features_batch_ 240  is appended\n",
            "features_batch_ 250  is appended\n",
            "features_batch_ 260  is appended\n",
            "features_batch_ 270  is appended\n",
            "features_batch_ 280  is appended\n",
            "features_batch_ 290  is appended\n",
            "features_batch_ 300  is appended\n",
            "features_batch_ 310  is appended\n",
            "features_batch_ 320  is appended\n",
            "features_batch_ 330  is appended\n",
            "features_batch_ 340  is appended\n",
            "features_batch_ 350  is appended\n",
            "features_batch_ 360  is appended\n",
            "features_batch_ 370  is appended\n",
            "features_batch_ 380  is appended\n",
            "features_batch_ 390  is appended\n",
            "features_batch_ 400  is appended\n",
            "features_batch_ 410  is appended\n",
            "features_batch_ 420  is appended\n",
            "features_batch_ 430  is appended\n",
            "features_batch_ 440  is appended\n",
            "features_batch_ 450  is appended\n",
            "features_batch_ 460  is appended\n",
            "features_batch_ 470  is appended\n",
            "features_batch_ 480  is appended\n",
            "features_batch_ 490  is appended\n",
            "features_batch_ 500  is appended\n",
            "features_batch_ 510  is appended\n",
            "features_batch_ 520  is appended\n",
            "features_batch_ 530  is appended\n",
            "features_batch_ 540  is appended\n",
            "features_batch_ 550  is appended\n",
            "features_batch_ 560  is appended\n",
            "features_batch_ 570  is appended\n",
            "features_batch_ 580  is appended\n",
            "features_batch_ 590  is appended\n",
            "features_batch_ 600  is appended\n",
            "features_batch_ 610  is appended\n",
            "features_batch_ 620  is appended\n",
            "features_batch_ 630  is appended\n",
            "features_batch_ 640  is appended\n",
            "features_batch_ 650  is appended\n",
            "features_batch_ 660  is appended\n",
            "features_batch_ 670  is appended\n",
            "features_batch_ 680  is appended\n",
            "features_batch_ 690  is appended\n",
            "features_batch_ 700  is appended\n",
            "features_batch_ 710  is appended\n",
            "features_batch_ 720  is appended\n",
            "features_batch_ 730  is appended\n",
            "features_batch_ 740  is appended\n",
            "features_batch_ 750  is appended\n",
            "features_batch_ 760  is appended\n",
            "features_batch_ 770  is appended\n",
            "features_batch_ 780  is appended\n",
            "features_batch_ 790  is appended\n",
            "features_batch_ 800  is appended\n",
            "features_batch_ 810  is appended\n",
            "features_batch_ 820  is appended\n",
            "features_batch_ 830  is appended\n",
            "features_batch_ 840  is appended\n",
            "features_batch_ 850  is appended\n",
            "features_batch_ 860  is appended\n",
            "features_batch_ 870  is appended\n",
            "features_batch_ 880  is appended\n",
            "features_batch_ 890  is appended\n",
            "features_batch_ 900  is appended\n",
            "features_batch_ 910  is appended\n",
            "features_batch_ 920  is appended\n",
            "features_batch_ 930  is appended\n",
            "features_batch_ 940  is appended\n",
            "features_batch_ 950  is appended\n",
            "features_batch_ 960  is appended\n",
            "features_batch_ 970  is appended\n",
            "features_batch_ 980  is appended\n",
            "features_batch_ 990  is appended\n",
            "features_batch_ 1000  is appended\n",
            "features_batch_ 1010  is appended\n",
            "features_batch_ 1020  is appended\n",
            "features_batch_ 1030  is appended\n",
            "features_batch_ 1040  is appended\n",
            "features_batch_ 1050  is appended\n",
            "features_batch_ 1060  is appended\n",
            "features_batch_ 1070  is appended\n",
            "features_batch_ 1080  is appended\n",
            "features_batch_ 1090  is appended\n",
            "features_batch_ 1100  is appended\n",
            "features_batch_ 1110  is appended\n",
            "features_batch_ 1120  is appended\n",
            "features_batch_ 1130  is appended\n",
            "features_batch_ 1140  is appended\n",
            "features_batch_ 1150  is appended\n",
            "features_batch_ 1160  is appended\n",
            "features_batch_ 1170  is appended\n",
            "features_batch_ 1180  is appended\n",
            "features_batch_ 1190  is appended\n",
            "features_batch_ 1200  is appended\n",
            "features_batch_ 1210  is appended\n",
            "features_batch_ 1220  is appended\n",
            "features_batch_ 1230  is appended\n",
            "features_batch_ 1240  is appended\n",
            "features_batch_ 1250  is appended\n",
            "features_batch_ 1260  is appended\n",
            "features_batch_ 1270  is appended\n",
            "features_batch_ 1280  is appended\n",
            "features_batch_ 1290  is appended\n",
            "features_batch_ 1300  is appended\n",
            "features_batch_ 1310  is appended\n",
            "features_batch_ 1320  is appended\n",
            "features_batch_ 1330  is appended\n",
            "features_batch_ 1340  is appended\n",
            "features_batch_ 1350  is appended\n",
            "features_batch_ 1360  is appended\n",
            "features_batch_ 1370  is appended\n",
            "features_batch_ 1380  is appended\n",
            "features_batch_ 1390  is appended\n",
            "features_batch_ 1400  is appended\n",
            "features_batch_ 1410  is appended\n",
            "features_batch_ 1420  is appended\n",
            "features_batch_ 1430  is appended\n",
            "features_batch_ 1440  is appended\n",
            "features_batch_ 1450  is appended\n",
            "features_batch_ 1460  is appended\n",
            "features_batch_ 1470  is appended\n",
            "features_batch_ 1480  is appended\n",
            "features_batch_ 1490  is appended\n",
            "features_batch_ 1500  is appended\n",
            "features_batch_ 1510  is appended\n",
            "features_batch_ 1520  is appended\n",
            "features_batch_ 1530  is appended\n",
            "features_batch_ 1540  is appended\n",
            "features_batch_ 1550  is appended\n",
            "features_batch_ 1560  is appended\n",
            "features_batch_ 1570  is appended\n",
            "features_batch_ 1580  is appended\n",
            "features_batch_ 1590  is appended\n",
            "features_batch_ 1600  is appended\n",
            "features_batch_ 1610  is appended\n",
            "features_batch_ 1620  is appended\n",
            "features_batch_ 1630  is appended\n",
            "features_batch_ 1640  is appended\n",
            "features_batch_ 1650  is appended\n",
            "features_batch_ 1660  is appended\n",
            "features_batch_ 1670  is appended\n",
            "features_batch_ 1680  is appended\n",
            "features_batch_ 1690  is appended\n",
            "features_batch_ 1700  is appended\n",
            "features_batch_ 1710  is appended\n",
            "features_batch_ 1720  is appended\n",
            "features_batch_ 1730  is appended\n",
            "features_batch_ 1740  is appended\n",
            "features_batch_ 1750  is appended\n",
            "features_batch_ 1760  is appended\n",
            "features_batch_ 1770  is appended\n",
            "features_batch_ 1780  is appended\n",
            "features_batch_ 1790  is appended\n",
            "features_batch_ 1800  is appended\n",
            "features_batch_ 1810  is appended\n",
            "features_batch_ 1820  is appended\n",
            "features_batch_ 1830  is appended\n",
            "features_batch_ 1840  is appended\n",
            "features_batch_ 1850  is appended\n",
            "features_batch_ 1860  is appended\n",
            "features_batch_ 1870  is appended\n",
            "features_batch_ 1880  is appended\n",
            "features_batch_ 1890  is appended\n",
            "features_batch_ 1900  is appended\n",
            "features_batch_ 1910  is appended\n",
            "features_batch_ 1920  is appended\n",
            "features_batch_ 1930  is appended\n",
            "features_batch_ 1940  is appended\n",
            "features_batch_ 1950  is appended\n",
            "features_batch_ 1960  is appended\n",
            "features_batch_1969 is appended\n",
            "All training features are grouped into a single NumPy array.\n",
            "Shape: (196860, 7, 7, 2048)\n",
            "Data type: float32\n"
          ]
        }
      ],
      "source": [
        "# Grouping training features into a single NumPy array.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Calculate the number of batches\n",
        "batch_size=100\n",
        "num_images = train_NoOfSamples\n",
        "num_batches = int(np.ceil(num_images / batch_size))\n",
        "#num_batches = 1969  # Total number of batches\n",
        "batch_interval = 10  # Interval between batches\n",
        "\n",
        "# Initialize an empty list to store features\n",
        "all_training_features = []\n",
        "\n",
        "# Iterate through each batch\n",
        "for i in range(1, num_batches + 1):\n",
        "    # Check if the current batch is within the desired interval\n",
        "    if i % batch_interval == 0:\n",
        "        # Load features from the file\n",
        "        #features_file = f\"./data/features_batch_{i}.npy\"\n",
        "        features_file = f\"D:/ResNet-152/features_batch_{i}.npy\"\n",
        "        batch_features = np.load(features_file).astype(np.float16)\n",
        "\n",
        "        # Append batch features to the list of all features\n",
        "        all_training_features.append(batch_features)\n",
        "        print(\"features_batch_\",str(i),\" is appended\")\n",
        "\n",
        "        if i == 1960: # the batch before the last batch\n",
        "            #features_file = f\"./data/features_batch_1969.npy\"\n",
        "            features_file = f\"D:/ResNet-152/features_batch_1969.npy\"\n",
        "            batch_features = np.load(features_file)\n",
        "            # Append batch features to the list of all features\n",
        "            all_training_features.append(batch_features)\n",
        "            print(\"features_batch_1969 is appended\")\n",
        "\n",
        "    \n",
        "\n",
        "# Concatenate all batches of features into a single NumPy array\n",
        "all_training_features = np.concatenate(all_training_features)\n",
        "\n",
        "# Save all features to a single NumPy array file\n",
        "output_file = \"./data/all_training_features.npy\"\n",
        "np.save(output_file, all_training_features)\n",
        "\n",
        "print(\"All training features are grouped into a single NumPy array.\")\n",
        "\n",
        "# The objects must then be expanded from a 3D array to a 4D array with the dimensions [samples, rows, cols, channels]\n",
        "print(\"Shape:\", all_training_features.shape) # VGG16 Shape: (196860, 7, 7, 512)\n",
        "print(\"Data type:\", all_training_features.dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All training features are load into a single NumPy array.\n",
            "Shape: (196860, 7, 7, 2048)\n",
            "Data type: float32\n"
          ]
        }
      ],
      "source": [
        "# Load all features to a single NumPy array file\n",
        "\n",
        "features_file = \"./data/all_training_features.npy\"\n",
        "all_training_features = np.load(features_file)\n",
        "\n",
        "print(\"All training features are load into a single NumPy array.\")\n",
        "\n",
        "# Verify the shape and data type of the loaded array\n",
        "print(\"Shape:\", all_training_features.shape) # VGG16 Shape: (196860, 7, 7, 2,048) = 19755294720\n",
        "print(\"Data type:\", all_training_features.dtype)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "N3KA4HGC0QiG"
      },
      "source": [
        "# Convert views to feature vectors\n",
        "\n",
        "https://towardsdatascience.com/image-similarity-detection-in-action-with-tensorflow-2-0-b8d9a78b2509\n",
        "\n",
        "An image feature vector is a list of numbers that represents a whole image, typically used for image similarity calculations or image classification tasks.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "n8QyDkbJ-Upw"
      },
      "source": [
        "## Feature vectors using flatten"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(196860, 100352)\n",
            "<class 'numpy.ndarray'>\n"
          ]
        }
      ],
      "source": [
        "#Run 3:\n",
        "########################\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Flatten each element in the array\n",
        "flattened_arr = all_training_features.reshape(all_training_features.shape[0], -1)\n",
        "\n",
        "# Update the original array with the flattened elements\n",
        "all_training_features = flattened_arr\n",
        "\n",
        "del flattened_arr\n",
        "\n",
        "# Print the shape of the updated array\n",
        "print(all_training_features.shape) # (196860, 100352)\n",
        "print(type(all_training_features))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n"
          ]
        }
      ],
      "source": [
        "# ensure that both are ndarray\n",
        "print(type(all_training_features))\n",
        "print(type(categorical_y_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "7qL8icJl6eBk"
      },
      "outputs": [],
      "source": [
        "# for training and testing\n",
        "##########################\n",
        "\n",
        "# to print the full NumPy array, without truncation\n",
        "\n",
        "import sys\n",
        "import numpy\n",
        "numpy.set_printoptions(threshold=sys.maxsize)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "oPfR-vMiXqx-"
      },
      "source": [
        "## Do the training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(196860, 100352)\n"
          ]
        }
      ],
      "source": [
        "# for training\n",
        "##########################\n",
        "# extract the nunmber of trained objects (samples) and thier number of features (features)\n",
        "\n",
        "print(all_training_features.shape)\n",
        "samples= all_training_features.shape[0]\n",
        "features= all_training_features.shape[1] # rows* cols* channels = 7*7*512 =25,088"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# shuffle in-place.\n",
        "# shuffle the all_training_features array in-place instead of creating a new shuffled array\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(42)\n",
        "np.random.shuffle(all_training_features)\n",
        "np.random.seed(42)\n",
        "np.random.shuffle(categorical_y_train)\n",
        "\n",
        "# In this modified code, the numpy.random.shuffle() function is used to shuffle the all_training_features and categorical_y_train arrays in-place. \n",
        "# By setting the random seed to the same value before shuffling both arrays, you ensure that the shuffling order is consistent between them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "######################################## Train FCN: 20 and 30 epochs###################################\n",
        "\n",
        "#import numpy as np\n",
        "#from keras.preprocessing import image\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "\n",
        "# use a pre-trained CNN to extract features from images in a dataset\n",
        "# and then feed the resulting feature vectors as input to a new fully-connected network\n",
        "\n",
        "i=0\n",
        "for deep_model in all_deep_models:\n",
        "    model_name = deep_model\n",
        "    model_name_txt = str(all_model_name_txt[i])\n",
        "    print (\"- Deep model: \"+str(model_name_txt))\n",
        "\n",
        "    FOLD_NAME = str(model_name_txt)+\".txt\"\n",
        "    Train_TimeFOLD_NAME = \"TrainingTime_FCN_stage2_\"+str(model_name_txt)+\".txt\"\n",
        "    \n",
        "\n",
        "# Define a new model as only fully-connected Network (FCN)\n",
        "    top_model = Sequential()\n",
        "\n",
        "    # new model as fully-connected network (FCN)\n",
        "    top_model.add(Dense(1024, activation='relu', input_dim=features)) # this layer is a fully connected layer with 1024 units and a ReLU activation function. The input_dim parameter specifies the input dimension of the layer,\n",
        "    top_model.add(Dropout(0.5)) #This layer is a dropout layer with a dropout rate of 0.5. Dropout is a regularization technique that randomly drops out (i.e., sets to zero) some of the units in the previous layer during each training epoch. This helps prevent overfitting and improve generalization.\n",
        "    top_model.add(Dense(40, activation='softmax')) # this layer is a fully connected layer with 40 units and a softmax activation function. The softmax function applies a normalization to the outputs of the previous layer, such that they sum up to one and can be interpreted as probabilities. This layer produces the final output of the network, which is a probability distribution over the 40 possible classes.\n",
        "\n",
        "\n",
        "    sgd = optimizers.gradient_descent_v2.SGD(lr = learning_rate, momentum=0.9, decay=0.01) # Stochastic gradient descent (often abbreviated SGD)\n",
        "    # Compile the new model\n",
        "    top_model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    print (\"# Final model architecture after update\")\n",
        "    top_model.summary()\n",
        "\n",
        "    print (\"# Final model architecture plotted\")\n",
        "    plot_model(top_model, to_file='model_2.png', rankdir='TB',show_dtype=True, show_layer_names=True)\n",
        "    ploted_model_path = \"./data/model_2.png\"  # Image file path\n",
        "    ploted_model_img = Image.open(ploted_model_path)\n",
        "    # save the fully-connected network (FCN)\n",
        "    ploted_model_new_path = \"./Results/\"+str(dataset_version)+\"/\"+str(model_name_txt)+\"/\"+str(model_name_txt)+\"_model_stage2_FCN.png\"\n",
        "\n",
        "\n",
        "    # FCN checkpoints paths\n",
        "    checkpoint_20_path = \"./Results/\"+str(dataset_version)+\"/\"+str(model_name_txt)+\"/\"+\"20epochs\"+\"/\"+str(model_name_txt)+\"_stage2_FCN.model\"\n",
        "    checkpoint_30_path = \"./Results/\"+str(dataset_version)+\"/\"+str(model_name_txt)+\"/\"+\"30epochs\"+\"/\"+str(model_name_txt)+\"_stage2_FCN.model\"\n",
        "\n",
        "\n",
        "    ###################### Start Training\n",
        "    \n",
        "    checkpoint_20 = ModelCheckpoint(checkpoint_20_path, monitor='accuracy', save_best_only=True, save_weights_only=False, mode='max', verbose=2)\n",
        "    checkpoint_30 = ModelCheckpoint(checkpoint_30_path, monitor='accuracy', save_best_only=True, save_weights_only=False, mode='max', verbose=2)\n",
        "    \n",
        "\n",
        "    STEPS_PER_EPOCH = 5\n",
        "    train_gen = DataGenerator(all_training_features, categorical_y_train, 32)\n",
        "    \n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Train the new model on the reshaped feature vectors\n",
        "    history = top_model.fit(train_gen, batch_size= BATCH_SIZE, epochs= 20, callbacks=[checkpoint_20], verbose=2)\n",
        "    # Calculate the training time for checkpoint 20 \n",
        "    training_time_20 = time.time() - start_time\n",
        "    print(\"Training time for checkpoint 20: \", training_time_20, \" seconds\")\n",
        "    PLOT_NAME = \"plot_acc_loss_20epochs_FCN\"+\".png\"\n",
        "    plot_acc_loss(history, PLOT_NAME, model_name_txt)\n",
        "\n",
        "    tt = \"./Results/\"+str(dataset_version)+\"/\"+str(model_name_txt)+\"/\"+\"20epochs\"+\"/\"+Train_TimeFOLD_NAME\n",
        "    f = open(tt, \"w\")\n",
        "    f.write(\"\\n\"+str(training_time_20))\n",
        "    f.close()\n",
        "    top_model.save(checkpoint_20_path)\n",
        "\n",
        "    # Continue training your model for the next 10 epochs (TOTAL 30), starting from the previously trained model\n",
        "    top_model = load_model(checkpoint_20_path)\n",
        "    history = top_model.fit(train_gen, batch_size= BATCH_SIZE, epochs= 10, callbacks=[checkpoint_30], verbose=2)\n",
        "    # Calculate the training time for checkpoint 30 \n",
        "    training_time_30 = time.time() - start_time\n",
        "    print(\"Training time for checkpoint 30: \", training_time_30, \" seconds\")\n",
        "    PLOT_NAME = \"plot_acc_loss_30epochs_FCN\"+\".png\"\n",
        "    plot_acc_loss(history, PLOT_NAME, model_name_txt)\n",
        "\n",
        "    tt = \"./Results/\"+str(dataset_version)+\"/\"+str(model_name_txt)+\"/\"+\"30epochs\"+\"/\"+Train_TimeFOLD_NAME\n",
        "    f = open(tt, \"w\")\n",
        "    f.write(\"\\n\"+str(training_time_30))\n",
        "    f.close()\n",
        "    top_model.save(checkpoint_30_path)\n",
        "\n",
        "    i+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "######################################## Train FCL: 20 and 30 epochs ###################################\n",
        "\n",
        "#import numpy as np\n",
        "#from keras.preprocessing import image\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "\n",
        "# use a pre-trained CNN to extract features from images in a dataset\n",
        "# and then feed the resulting feature vectors as input\n",
        "# to a new fully-connected network\n",
        "\n",
        "i=0\n",
        "for deep_model in all_deep_models:\n",
        "    model_name = deep_model\n",
        "    model_name_txt = str(all_model_name_txt[i])\n",
        "    print (\"- Deep model: \"+str(model_name_txt))\n",
        "\n",
        "    FOLD_NAME = str(model_name_txt)+\".txt\"\n",
        "    Train_TimeFOLD_NAME = \"TrainingTime_FCL_stage2_\"+str(model_name_txt)+\".txt\"\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "# Define a new model as only fully-connected Layer (FCL)\n",
        "    top_model = Sequential()\n",
        "\n",
        "    # new model as only one fully-connected layer (FCL)\n",
        "    top_model.add(Dense(40, activation='softmax', input_dim=features)) \n",
        "\n",
        "    sgd = optimizers.gradient_descent_v2.SGD(lr = learning_rate, momentum=0.9, decay=0.01) # Stochastic gradient descent (often abbreviated SGD)\n",
        "    # Compile the new model\n",
        "    top_model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    print (\"# Final model architecture after update\")\n",
        "    top_model.summary()\n",
        "\n",
        "    print (\"# Final model architecture plotted\")\n",
        "    plot_model(top_model, to_file='model_2.png', rankdir='TB',show_dtype=True, show_layer_names=True)\n",
        "    ploted_model_path = \"./data/model_2.png\"  # Image file path\n",
        "    ploted_model_img = Image.open(ploted_model_path)\n",
        "    # save the fully-connected layer (FCl)\n",
        "    ploted_model_new_path = \"./Results/\"+str(dataset_version)+\"/\"+str(model_name_txt)+\"/\"+str(model_name_txt)+\"_model_stage2_FCL.png\"\n",
        "    ploted_model_img.save(ploted_model_new_path) # Image saving to another directory\n",
        "\n",
        "\n",
        "\n",
        "    # FCL checkpoints paths\n",
        "    checkpoint_20_path = \"./Results/\"+str(dataset_version)+\"/\"+str(model_name_txt)+\"/\"+\"20epochs\"+\"/\"+str(model_name_txt)+\"_stage2_FCL.model\"\n",
        "    checkpoint_30_path = \"./Results/\"+str(dataset_version)+\"/\"+str(model_name_txt)+\"/\"+\"30epochs\"+\"/\"+str(model_name_txt)+\"_stage2_FCL.model\"\n",
        "\n",
        "    ###################### Start Training\n",
        "    checkpoint_20 = ModelCheckpoint(checkpoint_20_path, monitor='accuracy', save_best_only=True, save_weights_only=False, mode='max', verbose=2)\n",
        "    checkpoint_30 = ModelCheckpoint(checkpoint_30_path, monitor='accuracy', save_best_only=True, save_weights_only=False, mode='max', verbose=2)\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "    STEPS_PER_EPOCH = 5\n",
        "    train_gen = DataGenerator(all_training_features, categorical_y_train, 32)\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Train the new model on the reshaped feature vectors\n",
        "    history = top_model.fit(train_gen, batch_size= BATCH_SIZE, epochs= 20, callbacks=[checkpoint_20], verbose=2)\n",
        "    # Calculate the training time for checkpoint 20 \n",
        "    training_time_20 = time.time() - start_time\n",
        "    print(\"Training time for checkpoint 20: \", training_time_20, \" seconds\")\n",
        "    PLOT_NAME = \"plot_acc_loss_20epochs_FCL\"+\".png\"\n",
        "    plot_acc_loss(history, PLOT_NAME, model_name_txt)\n",
        "\n",
        "    tt = \"./Results/\"+str(dataset_version)+\"/\"+str(model_name_txt)+\"/\"+\"20epochs\"+\"/\"+Train_TimeFOLD_NAME\n",
        "    f = open(tt, \"w\")\n",
        "    f.write(\"\\n\"+str(training_time_20))\n",
        "    f.close()\n",
        "    top_model.save(checkpoint_20_path)\n",
        "\n",
        "    \n",
        "    # Continue training your model for the next 10 epochs (TOTAL 30), starting from the previously trained model\n",
        "    top_model = load_model(checkpoint_20_path)\n",
        "    history = top_model.fit(train_gen, batch_size= BATCH_SIZE, epochs= 10, callbacks=[checkpoint_30], verbose=2)\n",
        "    # Calculate the training time for checkpoint 30 \n",
        "    training_time_30 = time.time() - start_time\n",
        "    print(\"Training time for checkpoint 30: \", training_time_30, \" seconds\")\n",
        "    PLOT_NAME = \"plot_acc_loss_30epochs_FCL\"+\".png\"\n",
        "    plot_acc_loss(history, PLOT_NAME, model_name_txt)\n",
        "\n",
        "    tt = \"./Results/\"+str(dataset_version)+\"/\"+str(model_name_txt)+\"/\"+\"30epochs\"+\"/\"+Train_TimeFOLD_NAME\n",
        "    f = open(tt, \"w\")\n",
        "    f.write(\"\\n\"+str(training_time_30))\n",
        "    f.close()\n",
        "    top_model.save(checkpoint_30_path)\n",
        "\n",
        "    i+=1"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "047df207e8824458a6db1fc10c534962": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7b62b4432524026bce5aca6cd8d4c01",
            "placeholder": "",
            "style": "IPY_MODEL_0e2437200a814b65a0c3d98a8f913ebb",
            "value": " 468/468 [00:00&lt;00:00, 8344.85it/s]"
          }
        },
        "0e2437200a814b65a0c3d98a8f913ebb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "134884cf75604eee99efc15690d45831": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "152520a119574249b699e0cad2d92cf6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f01d40d83f8b48c6a24ccaf909f9a9b4",
            "placeholder": "",
            "style": "IPY_MODEL_b3e13e60a6c745629efa92a69f39756d",
            "value": "100%"
          }
        },
        "15cb7fe16498459fa81a518aa93e2daa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19526dd95ff3425b84440bf2bd81ff44": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1ac9fcc11e5d426d9a64fe2e36650e4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_59608b53a39b4d7d82660e8a57acfe13",
            "placeholder": "",
            "style": "IPY_MODEL_134884cf75604eee99efc15690d45831",
            "value": "100%"
          }
        },
        "2354607fa6864dc4837f537335d3efef": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f3c127d93444d638cd6ac099e0d841a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ec4b4268fbd49529dfc85bbee78a3b8",
            "max": 468,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c9963d8baaf44427872f872df308038c",
            "value": 468
          }
        },
        "4804a40c03ec4d5c8d374840c4b96673": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59608b53a39b4d7d82660e8a57acfe13": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ec4b4268fbd49529dfc85bbee78a3b8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79fd3b3fe3924bf5a4f996d0d81cbf49": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_152520a119574249b699e0cad2d92cf6",
              "IPY_MODEL_3f3c127d93444d638cd6ac099e0d841a",
              "IPY_MODEL_047df207e8824458a6db1fc10c534962"
            ],
            "layout": "IPY_MODEL_4804a40c03ec4d5c8d374840c4b96673"
          }
        },
        "87a338d78bf14e9789f853ee714922c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb73eccff262461889ab3009cb78bb4d",
            "placeholder": "",
            "style": "IPY_MODEL_19526dd95ff3425b84440bf2bd81ff44",
            "value": " 39/39 [00:00&lt;00:00, 1057.98it/s]"
          }
        },
        "9af925b95bd94a3f92dad82d3ca9e863": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_15cb7fe16498459fa81a518aa93e2daa",
            "max": 39,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_af5d3cf2b2f94e8a8002745ca7645f4f",
            "value": 39
          }
        },
        "af5d3cf2b2f94e8a8002745ca7645f4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b3e13e60a6c745629efa92a69f39756d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c7b62b4432524026bce5aca6cd8d4c01": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9963d8baaf44427872f872df308038c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "eb73eccff262461889ab3009cb78bb4d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f01d40d83f8b48c6a24ccaf909f9a9b4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3dd0375fb8b42f9b89c7200c1fe4903": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1ac9fcc11e5d426d9a64fe2e36650e4b",
              "IPY_MODEL_9af925b95bd94a3f92dad82d3ca9e863",
              "IPY_MODEL_87a338d78bf14e9789f853ee714922c6"
            ],
            "layout": "IPY_MODEL_2354607fa6864dc4837f537335d3efef"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
